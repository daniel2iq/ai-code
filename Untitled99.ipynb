{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxHhcSxYtj7Fpkh5KEbqUS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daniel2iq/ai-code/blob/main/Untitled99.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsfE08wHMgWf"
      },
      "outputs": [],
      "source": [
        "%pip install numpy pandas matplotlib seaborn\n",
        "%pip install scikit-learn\n",
        "!pip install category_encoders\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"adult.csv\")\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder, PolynomialFeatures, PowerTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from category_encoders import TargetEncoder\n",
        "from sklearn.model_selection import KFold, cross_val_score\n",
        "from scipy.stats import zscore\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_classif\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Print a greeting to verify code runs\n",
        "print(\"hello world\")\n",
        "\n",
        "# Show the first few rows of the dataset to see what we're working with\n",
        "df.head()\n",
        "\n",
        "# Find the number of missing values in the data so we can use simple imputer or other data imputation techniques\n",
        "df.isnull().sum()\n",
        "\n",
        "# Set features (X) to every column except income\n",
        "x = df.drop(\"income\", axis=1)\n",
        "\n",
        "# Set target variable (y) to the income column\n",
        "y = df[\"income\"]\n",
        "\n",
        "# Select numerical columns by dtype\n",
        "numerical_cols = x.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
        "\n",
        "# Select categorical columns by dtype\n",
        "categorical_cols = x.select_dtypes(include=\"object\").columns\n",
        "\n",
        "# Calculate absolute z-scores for numerical columns to identify outliers\n",
        "z_scores = np.abs(zscore(x[numerical_cols]))\n",
        "\n",
        "# Filter rows where all numerical z-scores are less than 3 standard deviations (remove outliers)\n",
        "filtered_df = df[z_scores < 3].all(axis=1)\n",
        "\n",
        "# Update x to only include filtered rows\n",
        "x = x[filtered_df]\n",
        "\n",
        "# Calculate skewness for numerical columns to identify skewed features\n",
        "skewed = np.abs(x[numerical_cols].skew())\n",
        "\n",
        "# Columns with skewness greater than 0.5 need transformation\n",
        "skewed_cols = [col for col in numerical_cols if skewed[col] > 0.5]\n",
        "\n",
        "# Non-skewed numerical columns\n",
        "non_skewed_cols = [col for col in numerical_cols if col not in skewed_cols]\n",
        "\n",
        "# Find the number of unique values in each categorical column\n",
        "unique_cols = x[categorical_cols].nunique()\n",
        "\n",
        "# Columns with fewer than 10 unique values (small unique)\n",
        "small_unique = [col for col in categorical_cols if unique_cols[col] < 10]\n",
        "\n",
        "# Columns with 10 or more unique values (big unique)\n",
        "big_unique = [col for col in categorical_cols if unique_cols[col] >= 10]\n",
        "\n",
        "# Pipeline to transform skewed numerical columns: power transform then scale\n",
        "skewed_cols_pipeline = Pipeline([\n",
        "    (\"power\", PowerTransformer(method=\"yeo-johnson\")),\n",
        "    (\"scale\", StandardScaler())\n",
        "])\n",
        "\n",
        "# Pipeline to scale non-skewed numerical columns\n",
        "non_skewed_cols_pipeline = Pipeline([\n",
        "    (\"scale\", StandardScaler())\n",
        "])\n",
        "\n",
        "# Pipeline for small unique categorical columns - one hot encoding\n",
        "small_unique_cols_pipeline = Pipeline([\n",
        "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "])\n",
        "\n",
        "# Pipeline for big unique categorical columns - target encoding\n",
        "big_unique_cols_pipeline = Pipeline([\n",
        "    (\"target\", TargetEncoder())\n",
        "])\n",
        "\n",
        "# Column transformer to apply respective pipelines to correct columns\n",
        "preprocessor = ColumnTransformer([\n",
        "    (\"skewed\", skewed_cols_pipeline, skewed_cols),\n",
        "    (\"non_skewed\", non_skewed_cols_pipeline, non_skewed_cols),\n",
        "    (\"small_unique\", small_unique_cols_pipeline, small_unique),\n",
        "    (\"big_unique\", big_unique_cols_pipeline, big_unique)\n",
        "])\n",
        "\n",
        "# Final pipeline: preprocessing, feature selection, polynomial features, and logistic regression model\n",
        "final_pipeline = Pipeline([\n",
        "    (\"preprocessor\", preprocessor),\n",
        "    (\"feature_selection\", SelectKBest(score_func=mutual_info_classif, k=10)),\n",
        "    (\"polynomial_features\", PolynomialFeatures(degree=2, include_bias=False, interaction_only=True)),\n",
        "    (\"model\", LogisticRegression())\n",
        "])\n",
        "\n",
        "# K-Fold cross-validation setup (5 splits, shuffle to avoid bias)\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Calculate cross-validation scores\n",
        "scores = cross_val_score(final_pipeline, x, y, cv=kf)\n",
        "\n",
        "print(f\"The non-skewed columns are: {non_skewed_cols}\")\n",
        "print(f\"Cross-validation scores: {scores}\")\n",
        "print(f\"The mean cross-validation score is: {scores.mean()}\")\n",
        "print(f\"The skewed columns are: {skewed_cols}\")\n",
        "\n",
        "print(\"Additional techniques I can use are:\")\n",
        "print(\"-\" * 40)\n",
        "print(\". CountVectorizer and TF-IDF vectorizer for text data\")\n",
        "print(\". Bayesian smoothing encoding to prevent data leakage\")\n",
        "print(\". Custom transformers to fit into pipelines seamlessly\")\n",
        "print(\". Feature crossing\")\n",
        "print(\". Dimensionality reduction techniques like PCA\")\n",
        "print(\". SimpleImputer, KNNImputer, and IterativeImputer for missing values\")\n",
        "print(\"BIG NOTE - I did not use any of these techniques here because they were not needed for this dataset.\")\n"
      ]
    }
  ]
}